{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVkFlWknLxI96nnY1Jucec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ouafighizlene21/classification_tid2013_cnn.ipynb/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¦ Importer les bibliothÃ¨ques nÃ©cessaires / Import Required Libraries  \n",
        "Importer les bibliothÃ¨ques essentielles pour la classification dâ€™images avec un modÃ¨le CNN,  \n",
        "y compris le traitement des images, la construction du modÃ¨le, l'entraÃ®nement, l'encodage des labels et l'affichage des rÃ©sultats.  \n",
        "Import essential libraries for image classification using a CNN model,  \n",
        "including image processing, model building, training, label encoding, and result visualization.\n"
      ],
      "metadata": {
        "id": "bzkv1Npo905B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HCBD4NlrcxNC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”— Monter Google Drive / Mount Google Drive  \n",
        "Connecter Google Colab Ã  Google Drive pour accÃ©der aux fichiers stockÃ©s.  \n",
        "Connect Google Colab to Google Drive to access stored files.\n"
      ],
      "metadata": {
        "id": "u0xTJTcdAPME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8EXAu_cc2Ch",
        "outputId": "5fccaba7-4951-44ea-dc8b-1ea8cf2aa1f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ—‚ï¸ DÃ©finir les chemins dâ€™accÃ¨s aux images / Set Image Paths  \n",
        "DÃ©finir les chemins des images de rÃ©fÃ©rence et des images distordues stockÃ©es dans Google Drive.  \n",
        "Set the paths to reference and distorted images stored in Google Drive.\n"
      ],
      "metadata": {
        "id": "xsEJW5xXAVnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DÃ©finir le chemin vers les images de rÃ©frence\n",
        "reference_dir = '/content/drive/My Drive/TID2013/reference_images'\n",
        "# DÃ©finir le chemin vers les images de rÃ©frence\n",
        "distorted_dir = '/content/drive/My Drive/TID2013/distorted_images'"
      ],
      "metadata": {
        "id": "Hf55PYgtc4Dj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ Charger et prÃ©parer les images / Load and Prepare Images  \n",
        "Charger les images de rÃ©fÃ©rence et distordues depuis leurs rÃ©pertoires respectifs, les redimensionner Ã  224x224 pixels,  \n",
        "les convertir en tableaux, les normaliser, puis attribuer une Ã©tiquette Ã  chaque image.  \n",
        "Load reference and distorted images from their respective directories, resize them to 224x224 pixels,  \n",
        "convert them to arrays, normalize them, and assign a label to each image.\n"
      ],
      "metadata": {
        "id": "xpOO7Od3Aeif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tid2013(data_dir, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img_file in os.listdir(data_dir):\n",
        "        img_path = os.path.join(data_dir, img_file)\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img = image.img_to_array(img)\n",
        "        img = img / 255.0\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "ref_images, ref_labels = load_tid2013(reference_dir, 'reference')\n",
        "dist_images, dist_labels = load_tid2013(distorted_dir, 'distorted')\n",
        "\n",
        "# Combiner les donnÃ©es des deux rÃ©pertoires\n",
        "images = np.array(ref_images + dist_images)\n",
        "labels = np.array(ref_labels + dist_labels)"
      ],
      "metadata": {
        "id": "Xrqc3dYZc6ap"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ‚ï¸ Diviser les donnÃ©es en ensembles dâ€™entraÃ®nement et de test / Split Data into Training and Testing Sets  \n",
        "Diviser les images et leurs Ã©tiquettes en un ensemble dâ€™entraÃ®nement (80%) et un ensemble de test (20%) pour Ã©valuer les performances du modÃ¨le.  \n",
        "Split the images and their labels into a training set (80%) and a testing set (20%) to evaluate model performance.\n"
      ],
      "metadata": {
        "id": "kc3rO4R8BI8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zMGlO5hNdDlg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”¤ Encoder les Ã©tiquettes / Encode Labels  \n",
        "Utiliser `LabelEncoder` pour convertir les Ã©tiquettes textuelles (comme \"reference\" et \"distorted\") en valeurs numÃ©riques.  \n",
        "`fit_transform` est appliquÃ© sur les donnÃ©es d'entraÃ®nement pour ajuster et transformer, tandis que `transform` est utilisÃ© sur les donnÃ©es de test.  \n",
        "Use `LabelEncoder` to convert textual labels (e.g., \"reference\" and \"distorted\") into numeric values.  \n",
        "Apply `fit_transform` on the training set to fit and transform simultaneously, and use `transform` on the test set to apply the learned mapping.\n"
      ],
      "metadata": {
        "id": "OS393X3gBTIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#utilisez fit_transform sur les donnÃ©es d'entraÃ®nement pour ajuster et transformer simultanÃ©ment, et utilisez transform sur les donnÃ©es de test ou toute nouvelle donnÃ©e pour appliquer la transformation apprise.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Initialiser l'encodeur\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encoder les Ã©tiquettes d'entraÃ®nement\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "# Encoder les Ã©tiquettes de test\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Afficher les classes apprises par l'encodeur\n",
        "print(\"Classes apprises par l'encodeur :\", le.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDshQFiTyoYM",
        "outputId": "5dc15b4b-559e-4c02-b2fd-0e3dbce78449"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes apprises par l'encodeur : ['distorted' 'reference']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  Construire et compiler le modÃ¨le CNN / Build and Compile the CNN Model  \n",
        "Encoder les Ã©tiquettes en vecteurs one-hot pour la classification multiclasse.  \n",
        "Construire un modÃ¨le CNN avec des couches de convolution, de max-pooling, une couche flatten,  \n",
        "et des couches denses entiÃ¨rement connectÃ©es. Compiler ensuite le modÃ¨le avec l'optimiseur Adam  \n",
        "et la fonction de perte `categorical_crossentropy`.  \n",
        "Encode the labels using one-hot encoding for multi-class classification.  \n",
        "Build a CNN model with convolutional layers, max-pooling, a flatten layer,  \n",
        "and fully connected dense layers. Then compile the model using the Adam optimizer  \n",
        "and the `categorical_crossentropy` loss function.\n"
      ],
      "metadata": {
        "id": "PWOrJTC9Bms2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodage one-hot des labels\n",
        "y_train_one_hot = to_categorical(y_train_encoded, num_classes=2)\n",
        "y_test_one_hot = to_categorical(y_test_encoded, num_classes=2)\n",
        "\n",
        "# CrÃ©ation du modÃ¨le\n",
        "model = Sequential()\n",
        "\n",
        "# Convolution + MaxPooling\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Aplatir les sorties de la derniÃ¨re couche de convolution\n",
        "model.add(Flatten())\n",
        "\n",
        "# Couches entiÃ¨rement connectÃ©es\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))  # 2 neurones avec softmax pour multi-classes\n",
        "\n",
        "# Compilation du modÃ¨le\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Afficher le rÃ©sumÃ© du modÃ¨le\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "GgOR8EmGoKOr",
        "outputId": "97741be5-14e4-42cd-a9ca-70248840d8a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚    \u001b[38;5;34m11,075,712\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚           \u001b[38;5;34m258\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,169,218\u001b[0m (42.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,218</span> (42.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,169,218\u001b[0m (42.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,218</span> (42.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš€ EntraÃ®ner le modÃ¨le CNN / Train the CNN Model  \n",
        "EntraÃ®ner le modÃ¨le sur lâ€™ensemble d'entraÃ®nement pendant 10 Ã©poques avec une taille de lot de 32.  \n",
        "Les performances sont validÃ©es Ã  chaque Ã©poque sur lâ€™ensemble de test.  \n",
        "Train the model on the training set for 10 epochs with a batch size of 32.  \n",
        "Performance is validated at each epoch on the test set.\n"
      ],
      "metadata": {
        "id": "bEe7NzGaHwuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EntraÃ®nement du modÃ¨le\n",
        "history = model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1cjUMvF48hl",
        "outputId": "4b8644b0-77f2-4cc9-f18c-8efd55399522"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 4s/step - accuracy: 0.9877 - loss: 0.0991 - val_accuracy: 0.9934 - val_loss: 0.0448\n",
            "Epoch 2/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 4s/step - accuracy: 0.9918 - loss: 0.0545 - val_accuracy: 0.9934 - val_loss: 0.0413\n",
            "Epoch 3/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 4s/step - accuracy: 0.9892 - loss: 0.0676 - val_accuracy: 0.9934 - val_loss: 0.0483\n",
            "Epoch 4/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3s/step - accuracy: 0.9911 - loss: 0.0613 - val_accuracy: 0.9934 - val_loss: 0.0451\n",
            "Epoch 5/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 3s/step - accuracy: 0.9931 - loss: 0.0475 - val_accuracy: 0.9934 - val_loss: 0.0528\n",
            "Epoch 6/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3s/step - accuracy: 0.9906 - loss: 0.0591 - val_accuracy: 0.9934 - val_loss: 0.0433\n",
            "Epoch 7/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 3s/step - accuracy: 0.9915 - loss: 0.0514 - val_accuracy: 0.9934 - val_loss: 0.0419\n",
            "Epoch 8/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 3s/step - accuracy: 0.9930 - loss: 0.0427 - val_accuracy: 0.9934 - val_loss: 0.0456\n",
            "Epoch 9/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 3s/step - accuracy: 0.9922 - loss: 0.0461 - val_accuracy: 0.9934 - val_loss: 0.0511\n",
            "Epoch 10/10\n",
            "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 3s/step - accuracy: 0.9933 - loss: 0.0429 - val_accuracy: 0.9934 - val_loss: 0.0520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¾ Sauvegarder le modÃ¨le et lâ€™historique d'entraÃ®nement / Save the Model and Training History  \n",
        "Sauvegarder le modÃ¨le entraÃ®nÃ© au format `.keras` pour pouvoir le recharger ultÃ©rieurement sans le rÃ©entraÃ®ner.  \n",
        "Enregistrer Ã©galement l'historique d'entraÃ®nement (prÃ©cision, perte, etc.) dans un fichier `.pkl`  \n",
        "pour pouvoir visualiser les courbes plus tard.  \n",
        "Save the trained model in `.keras` format to reload it later without retraining.  \n",
        "Also save the training history (accuracy, loss, etc.) in a `.pkl` file to visualize the curves later.\n"
      ],
      "metadata": {
        "id": "sQ04pfUpTCt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Sauvegarder l'historique d'entraÃ®nement\n",
        "with open('historique_entraÃ®nement.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "# Sauvegarde du modÃ¨le dans le nouveau format .keras\n",
        "model.save('mon_modele_entraine.keras')\n"
      ],
      "metadata": {
        "id": "whrvhtJATQQn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”„ Recharger lâ€™historique dâ€™entraÃ®nement / Reload Training History  \n",
        "Charger les mÃ©triques dâ€™entraÃ®nement prÃ©cÃ©demment sauvegardÃ©es (prÃ©cision, perte, etc.)  \n",
        "depuis un fichier `.pkl` pour permettre leur visualisation sans relancer l'entraÃ®nement.  \n",
        "Load previously saved training metrics (accuracy, loss, etc.)  \n",
        "from a `.pkl` file to allow visualization without retraining.\n"
      ],
      "metadata": {
        "id": "EwT8nbjZVaMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recharger l'historique\n",
        "with open('historique_entraÃ®nement.pkl', 'rb') as f:\n",
        "    history_dict = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "AVyDwvCvUNHc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“‚ Charger le modÃ¨le sauvegardÃ© / Load the Saved Model  \n",
        "Recharger le modÃ¨le CNN sauvegardÃ© au format `.keras` afin de lâ€™utiliser pour lâ€™Ã©valuation ou les prÃ©dictions,  \n",
        "sans avoir Ã  le rÃ©entraÃ®ner.  \n",
        "Reload the CNN model saved in `.keras` format to use it for evaluation or predictions,  \n",
        "without retraining it.\n"
      ],
      "metadata": {
        "id": "CDY9Yne-VfI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modÃ¨le sauvegardÃ©\n",
        "model = load_model('mon_modele_entraine.keras')\n"
      ],
      "metadata": {
        "id": "6HPsDUH0FON3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ PrÃ©parer une image pour la prÃ©diction / Prepare an Image for Prediction  \n",
        "DÃ©finir une fonction pour charger une image, la redimensionner, la convertir en tableau numpy,  \n",
        "la normaliser et lui ajouter une dimension pour lâ€™adapter Ã  lâ€™entrÃ©e du modÃ¨le.  \n",
        "Define a function to load an image, resize it, convert it to a NumPy array,  \n",
        "normalize it, and add a batch dimension to fit the model input.\n"
      ],
      "metadata": {
        "id": "wkGlhEG6VkVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour prÃ©parer l'image\n",
        "def prepare_image(img_path, target_size=(224, 224)):\n",
        "    img = image.load_img(img_path, target_size=target_size)  # Charger l'image\n",
        "    img_array = image.img_to_array(img)  # Convertir en tableau numpy\n",
        "    img_array = img_array / 255.0  # Normaliser\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Ajouter une dimension pour le batch\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "DuAbEC6LJOSJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ·ï¸ Afficher les classes apprises par lâ€™encodeur / Show Classes Learned by the Encoder  \n",
        "Afficher lâ€™ordre des classes (Ã©tiquettes) que le `LabelEncoder` a appris lors de lâ€™encodage des donnÃ©es.  \n",
        "Cela permet de connaÃ®tre la correspondance exacte entre les indices numÃ©riques (0, 1, ...) et les Ã©tiquettes textuelles (`reference`, `distorted`, etc.).  \n",
        "Display the order of class labels learned by the `LabelEncoder` when encoding data.  \n",
        "This ensures correct mapping between numeric indices (0, 1, ...) and textual labels (`reference`, `distorted`, etc.).\n"
      ],
      "metadata": {
        "id": "rIQ-ZReej6DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classes connues par l'encodeur :\", le.classes_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY4wq8cMY2bT",
        "outputId": "eaa64ddf-4a5b-4258-a5fa-4dace0671e97"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes connues par l'encodeur : ['distorted' 'reference']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ” PrÃ©dire la classe dâ€™une image avec le modÃ¨le entraÃ®nÃ© / Predict the Class of an Image Using the Trained Model  \n",
        "Charger une image issue du dossier des images distordues, la prÃ©parer, et lâ€™utiliser comme entrÃ©e pour le modÃ¨le CNN.  \n",
        "La prÃ©diction est ensuite interprÃ©tÃ©e en utilisant les classes apprises automatiquement par le `LabelEncoder`,  \n",
        "ce qui garantit la cohÃ©rence entre lâ€™index prÃ©dit et le nom de la classe (`distorted`, `reference`, etc.).  \n",
        "Load a distorted image, preprocess it, and use it as input to the trained CNN model.  \n",
        "The predicted class index is mapped back to the actual class name using the labels learned by the `LabelEncoder`,  \n",
        "ensuring consistency between prediction index and true label (`distorted`, `reference`, etc.).\n"
      ],
      "metadata": {
        "id": "FNP_LYlPj-eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PrÃ©parer l'image pour la prÃ©diction\n",
        "img_path = '/content/drive/My Drive/TID2013/distorted_images/i01_18_4.jpg'\n",
        "prepared_image = prepare_image(img_path)\n",
        "\n",
        "# Faire la prÃ©diction\n",
        "predictions = model.predict(prepared_image)\n",
        "\n",
        "# Utiliser directement les classes de l'encodeur\n",
        "predicted_label = le.classes_[np.argmax(predictions)]\n",
        "\n",
        "print(f\"Classe prÃ©dite : {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5BXvPlWYxNO",
        "outputId": "ad0717c9-79b5-4e9c-a88c-19965bacde87"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "Classe prÃ©dite : distorted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  PrÃ©dire la classe dâ€™une image avec le modÃ¨le entraÃ®nÃ© / Predict the Class of an Image Using the Trained Model  \n",
        "Charger une image issue du dossier des images distordues, la prÃ©parer, et lâ€™utiliser comme entrÃ©e pour le modÃ¨le CNN.  \n",
        "La prÃ©diction est ensuite interprÃ©tÃ©e en utilisant les classes apprises automatiquement par le `LabelEncoder`,  \n",
        "ce qui garantit la cohÃ©rence entre lâ€™index prÃ©dit et le nom de la classe (`distorted`, `reference`, etc.).  \n",
        "Load a distorted image, preprocess it, and use it as input to the trained CNN model.  \n",
        "The predicted class index is mapped back to the actual class name using the labels learned by the `LabelEncoder`,  \n",
        "ensuring consistency between prediction index and true label (`distorted`, `reference`, etc.).\n",
        "\n",
        "âš ï¸ **Remarque importante / Important Note:**  \n",
        "Lâ€™image utilisÃ©e pour ce test provient du **dossier `reference_images`**,  \n",
        "mais le modÃ¨le a prÃ©dit Ã  tort quâ€™elle appartient Ã  la classe `\"distorted\"`.  \n",
        "Cela montre que le modÃ¨le peut faire des erreurs, probablement Ã  cause du fort dÃ©sÃ©quilibre entre les classes lors de l'entraÃ®nement.  \n",
        "The image used for this test comes from the **`reference_images` folder**,  \n",
        "but the model mistakenly predicted it as belonging to the `\"distorted\"` class.  \n",
        "This highlights that the model can make mistakes, likely due to strong class imbalance during training.\n"
      ],
      "metadata": {
        "id": "e4tQqXDosSqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PrÃ©parer l'image pour la prÃ©diction\n",
        "img_path = '/content/drive/My Drive/TID2013/reference_images/I11.jpg'  # Chemin de l'image\n",
        "prepared_image = prepare_image(img_path)\n",
        "\n",
        "# Faire la prÃ©diction\n",
        "predictions = model.predict(prepared_image)\n",
        "\n",
        "# Utiliser le mÃªme ordre de classes que l'encodeur\n",
        "class_labels = list(le.classes_)\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "print(f\"Classe prÃ©dite : {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyfPBOVBYy5y",
        "outputId": "0bce7fcc-35d0-4cc1-dc99-8cd172086ba9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Classe prÃ©dite : distorted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš ï¸ Attention : correspondance des classes et prÃ©dictions  \n",
        "## âš ï¸ Warning: Class Mapping and Predictions\n",
        "\n",
        "Lorsque vous utilisez `np.argmax(predictions)` pour obtenir lâ€™indice de la classe prÃ©dite,  \n",
        "il est **essentiel dâ€™utiliser les vraies Ã©tiquettes apprises par le `LabelEncoder`**,  \n",
        "et **pas une liste manuelle comme `['reference', 'distorted']`**.\n",
        "\n",
        "When using `np.argmax(predictions)` to get the predicted class index,  \n",
        "it is **crucial to use the actual class labels learned by the `LabelEncoder`**,  \n",
        "and **not a manually defined list like `['reference', 'distorted']`**.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¡ Mauvais exemple / Bad example:\n",
        "```python\n",
        "class_labels = ['reference', 'distorted']\n",
        "predicted_label = class_labels[np.argmax(predictions)]\n"
      ],
      "metadata": {
        "id": "eSZgzdiBrb1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PrÃ©parer l'image pour la prÃ©diction\n",
        "img_path = '/content/drive/My Drive/TID2013/reference_images/I11.jpg'  # Chemin de l'image\n",
        "prepared_image = prepare_image(img_path)\n",
        "\n",
        "# Faire la prÃ©diction\n",
        "predictions = model.predict(prepared_image)\n",
        "\n",
        "# Utiliser le mÃªme ordre de classes que l'encodeur\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "class_labels = ['reference', 'distorted']  # Les Ã©tiquettes de tes classes\n",
        "predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "print(f\"Classe prÃ©dite : {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXs-60YUhs9W",
        "outputId": "fe17d3ea-0fc2-42d0-ff60-af21e0ce1cbf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Classe prÃ©dite : reference\n"
          ]
        }
      ]
    }
  ]
}